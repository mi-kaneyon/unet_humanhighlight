# unet_humanhighlight
background make all black mask
# Human Segmentation with U-Net

This repository provides a pipeline for real-time human segmentation using a U-Net model trained on pseudo-labels generated by a pre-trained DeepLabV3 model. The workflow includes:

1. **Data Collection (U-Net ready)**: Use DeepLabV3 to capture images from a webcam and automatically generate segmentation masks, saved at a resolution consistent with U-Net training.
2. **Model Training**: Train a U-Net model on the collected dataset.
3. **Evaluation & Visualization**: Compare the predicted masks with the ground-truth (pseudo-labeled) masks.
4. **Real-Time Inference**: Run the trained U-Net model on live webcam input for real-time segmentation, using the same resolution and normalization settings as in training.

---

## Prerequisites

- **Python**: 3.9 or later
- **GPU (recommended)**: For faster training and inference
- **Dependencies**: Install from `requirements.txt`
  
  ```bash
  pip install -r requirements.txt
  ```

## projct tree

 ```bash
├── data
│   ├── images
│   └── masks
├── models
└── scripts
    ├── data_collection.py
    ├── real_train.py
    ├── real_time_segmentation.py
    ├── compare_performance.py
    ├── compare_3pics.py
    ├── unet.py
    └── ...

```
- data/images and data/masks: Will hold training images and corresponding masks (e.g., 512x512).
- models: Will store the trained U-Net model weights (unet_model.pth).
- scripts: Contains Python scripts for each step of the pipeline.

```bash
git clone <your-repo-url>
cd <your-repo-name>

```
### Setup directories (optional):
- If you have a script (e.g., setup_project.sh) to create directories and move scripts:


```bash

./setup_project.sh

```
**Otherwise, create directories manually if needed: **
```bash

mkdir -p data/images data/masks models scripts

```

### Move your .py files into scripts/ if they aren't already

# Usage
# Steps to Run the Pipeline

## 1. Data Collection (U-Net Ready)

**Script:** `data_collection.py`  
**Purpose:** Capture images from your webcam, generate pseudo-labeled masks using DeepLabV3, and save them at the U-Net training resolution (e.g., 512x512).

```bash
python scripts/data_collection.py

```

    - Opens the webcam and captures frames.
    - Runs DeepLabV3 to generate human segmentation masks.
    - Saves images and masks into data/images and data/masks, respectively, at the chosen resolution.
    - Press q to stop data collection.

> [!NOTE]
>: Ensure that the data_collection.py script has been modified to generate data compatible with U-Net (same resolution, e.g., 512x512).

2. Model Training

**Script: real_train.py**
- Purpose: Train the U-Net model on the collected dataset.

```bash
python scripts/real_train.py


```
> [!RIP]
### What it does:

    - Reads configuration from config.yaml (e.g., batch size, learning rate, epochs).
    - Loads training images and masks from data/.
    - Trains U-Net and saves unet_model.pth into models/.

> [!NOTE]
> Adjust config.yaml as needed for hyperparameters and paths.




3. Evaluation & Visualization

**Script: compare_performance.py or compare_3pics.py**
> Purpose: Compare the predicted U-Net masks with the ground-truth (pseudo-labeled) masks.

```bash
python scripts/compare_performance.py

```
> [!TIP]
### What it does:

    - Iterates through data/images and data/masks.
    - Uses the trained U-Net model (unet_model.pth) to generate predictions.
    - Displays comparisons: Input Image vs Ground Truth Mask vs Predicted Mask.

### For alternative visualization approaches, use:

```bash
python scripts/compare_3pics.py

```

4. Real-Time Inference

**Script: real_time_segmentation.py**
- Purpose: Run the U-Net model on live webcam input and visualize the segmentation results in real-time at the same resolution used during training.

```bash
python scripts/real_time_segmentation.py

```


> [!TIP]
> ### What it does:
> - Opens your webcam feed.
> - Resizes frames to the training resolution (e.g., 512x512), performs inference with U-Net.
> - Resizes the predicted mask back to the original frame size and displays it.
> - Press q to quit.

